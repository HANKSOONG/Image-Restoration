{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP2FyE1Ivc54zWaKFgc2N6c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HANKSOONG/image-repair-and-restoration-/blob/main/DnCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "EIOMn1_Go9nG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbce7355-e66f-4e5e-cd3d-3df189f67301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/Colab Notebooks/GOPRO_Large.zip\" -d \"/content/datasets\""
      ],
      "metadata": {
        "id": "Nbghz20UpAxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "cpu_count = os.cpu_count()\n",
        "\n",
        "print(f\"Number of CPU cores: {cpu_count}\")"
      ],
      "metadata": {
        "id": "QJA3cX8hqjFb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3294c6a4-1d41-4300-a1b2-6e7b137fceef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of CPU cores: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from PIL import Image\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = os.path.expanduser(root_dir)\n",
        "        self.transform = transform\n",
        "        self.samples = self._load_samples()\n",
        "\n",
        "    def _load_samples(self):\n",
        "        samples = []\n",
        "        for subdir in sorted(os.listdir(self.root_dir)):\n",
        "            subdir_path = os.path.join(self.root_dir, subdir)\n",
        "            if os.path.isdir(subdir_path):\n",
        "                sharp_imgs = sorted(os.listdir(os.path.join(subdir_path, 'sharp')))\n",
        "                blur_imgs = sorted(os.listdir(os.path.join(subdir_path, 'blur')))\n",
        "\n",
        "                for sharp_img, blur_img in zip(sharp_imgs, blur_imgs):\n",
        "                    sharp_path = os.path.join(subdir_path, 'sharp', sharp_img)\n",
        "                    blur_path = os.path.join(subdir_path, 'blur', blur_img)\n",
        "                    samples.append((sharp_path, blur_path))\n",
        "        return samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sharp_path, blur_path = self.samples[idx]\n",
        "        sharp_img_original = Image.open(sharp_path).convert('RGB')\n",
        "        sharp_img = sharp_img_original.copy()  # Create a copy for transformation\n",
        "        blur_img = Image.open(blur_path).convert('RGB')\n",
        "\n",
        "        sharp_img_original = transforms.ToTensor()(sharp_img_original)\n",
        "\n",
        "        if self.transform:\n",
        "            sharp_img = self.transform(sharp_img)\n",
        "            blur_img = self.transform(blur_img)\n",
        "\n",
        "        return blur_img, sharp_img, sharp_img_original\n",
        "\n",
        "# transform function\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((360, 640)), # Lower resolution\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "train_dataset = CustomDataset(root_dir='/content/datasets/train', transform=transform)\n",
        "\n",
        "# Split the data set into training set and validation set\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_subset,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    num_workers=8\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_subset,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    num_workers=8\n",
        ")"
      ],
      "metadata": {
        "id": "E7gVWCr1o8Up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DO4PmSemxtb"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n",
        "import torch.distributed as distance\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#def Perceptual Loss by MobileNet\n",
        "class PerceptualLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PerceptualLoss, self).__init__()\n",
        "        self.mobilenet = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.DEFAULT).features\n",
        "        self.mobilenet.eval()\n",
        "        for param in self.mobilenet.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        input_features = self.mobilenet(input)\n",
        "        target_features = self.mobilenet(target)\n",
        "        # Resize if necessary\n",
        "        if input_features.shape[2:] != target_features.shape[2:]:\n",
        "            input_features = F.interpolate(input_features, size=target_features.shape[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "        loss = nn.functional.mse_loss(input_features, target_features)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the DnCNN Model\n",
        "class DnCNN(nn.Module):\n",
        "    def __init__(self, channels, num_of_layers=17):\n",
        "        super(DnCNN, self).__init__()\n",
        "        kernel_size = 3\n",
        "        padding = 1\n",
        "        features = 64\n",
        "        layers = []\n",
        "\n",
        "        # Initial convolution layer\n",
        "        layers.append(nn.Conv2d(in_channels=channels, out_channels=features, kernel_size=kernel_size, padding=padding, bias=False))\n",
        "        layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        # Middle layers\n",
        "        for _ in range(num_of_layers - 2):\n",
        "            layers.append(nn.Conv2d(in_channels=features, out_channels=features, kernel_size=kernel_size, padding=padding, bias=False))\n",
        "            layers.append(nn.BatchNorm2d(features))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        # Final convolution layer\n",
        "        layers.append(nn.Conv2d(in_channels=features, out_channels=channels, kernel_size=kernel_size, padding=padding, bias=False))\n",
        "\n",
        "        self.dncnn = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.dncnn(x)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "KYFgVY3znnYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "# Creat model and Adam optimizer\n",
        "denoising_model = DnCNN(channels=3).to(device)\n",
        "mse_criterion = nn.MSELoss()\n",
        "perceptual_criterion = PerceptualLoss().to(device)\n",
        "optimizer = torch.optim.Adam(denoising_model.parameters(), lr=0.0001)\n",
        "\n",
        "# Initialize the ReduceLROnPlateau scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=3, verbose=True)"
      ],
      "metadata": {
        "id": "EFXjGtrjnzpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining Loss Function Weights\n",
        "mse_weight = 1.0\n",
        "perceptual_weight = 0.1"
      ],
      "metadata": {
        "id": "ObUTtA3X3zj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "def train_model(model, train_loader, val_loader, mse_criterion, perceptual_criterion, optimizer, num_epochs=100, early_stopping_tolerance=8):\n",
        "    best_val_loss = float('inf')\n",
        "    no_improvement_count = 0  # Early stopping counter\n",
        "\n",
        "    scaler = GradScaler()\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for blur_img, transformed_sharp_img, _ in train_loader:\n",
        "            blur_img = blur_img.to(device)\n",
        "            transformed_sharp_img = transformed_sharp_img.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "           # Performing forward propagation using the autocast context\n",
        "            with autocast():\n",
        "                outputs = model(blur_img)\n",
        "                mse_loss = mse_criterion(outputs, transformed_sharp_img)\n",
        "                perceptual_loss = perceptual_criterion(outputs, transformed_sharp_img)\n",
        "                total_loss = mse_weight * mse_loss + perceptual_weight * perceptual_loss\n",
        "\n",
        "            # Performing backward propagation and optimization using GradScaler\n",
        "            scaler.scale(total_loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += total_loss.item() * blur_img.size(0)\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "        # Validation test\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for blur_img, transformed_sharp_img, _ in val_loader:\n",
        "                blur_img = blur_img.to(device)\n",
        "                transformed_sharp_img = transformed_sharp_img.to(device)\n",
        "                outputs = model(blur_img)\n",
        "                mse_loss = mse_criterion(outputs, transformed_sharp_img)\n",
        "                perceptual_loss = perceptual_criterion(outputs, transformed_sharp_img)\n",
        "                total_loss = mse_weight * mse_loss + perceptual_weight * perceptual_loss\n",
        "                val_loss += total_loss.item() * blur_img.size(0)\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        # Early stopping check\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            no_improvement_count = 0\n",
        "        else:\n",
        "            no_improvement_count += 1\n",
        "            if no_improvement_count >= early_stopping_tolerance:\n",
        "                print(\"Stopping early due to no improvement in validation loss\")\n",
        "                break\n",
        "\n",
        "    return model\n",
        "\n",
        "train_model(denoising_model, train_loader, val_loader, mse_criterion, perceptual_criterion, optimizer, num_epochs=100, early_stopping_tolerance=8)"
      ],
      "metadata": {
        "id": "F2tZWSpooAbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "if torch.cuda.is_available() and torch.cuda.current_device() == 0:\n",
        "    model_path = '/content/drive/MyDrive/model/DnCNN/denoising_DnCNN.pth'\n",
        "    model_dir = os.path.expanduser(os.path.dirname(model_path))\n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "\n",
        "    torch.save(denoising_model.state_dict(), os.path.expanduser(model_path))\n",
        "    print(f\"Model saved to {model_path}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbYdMwQvmUfv",
        "outputId": "524d1f70-6422-4ce5-8ac1-c090e1478e37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/drive/MyDrive/model/DnCNN/denoising_DnCNN.pth.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inverse normalization transformation\n",
        "inv_normalize = transforms.Normalize(\n",
        "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
        "    std=[1/0.229, 1/0.224, 1/0.225]\n",
        ")"
      ],
      "metadata": {
        "id": "bL2CvvBUR07u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
        "from skimage.metrics import structural_similarity as compare_ssim\n",
        "from skimage import img_as_float\n",
        "import torch\n",
        "\n",
        "# Initialize the sum of PSNR and SSIM\n",
        "total_psnr_sharp = 0\n",
        "total_ssim_sharp = 0\n",
        "total_psnr_blur = 0\n",
        "total_ssim_blur = 0\n",
        "num_images = 0\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_path = '/content/drive/MyDrive/model/DnCNN/denoising_DnCNN3.pth'\n",
        "\n",
        "model = DnCNN(channels=3)\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for blur_img, sharp_img, _ in val_loader:\n",
        "        blur_img = blur_img.to(device)\n",
        "        sharp_img = sharp_img.to(device)\n",
        "\n",
        "        denoised_img = model(blur_img)\n",
        "\n",
        "# Traverse the image to calculate PSNR and SSIM\n",
        "        for i in range(blur_img.size(0)):\n",
        "            denoised = inv_normalize(denoised_img[i]).clamp(0, 1)\n",
        "            sharp = inv_normalize(sharp_img[i]).clamp(0, 1)\n",
        "            blur = inv_normalize(blur_img[i]).clamp(0, 1)\n",
        "\n",
        "            denoised_np = denoised.cpu().numpy().transpose(1, 2, 0)\n",
        "            sharp_np = sharp.cpu().numpy().transpose(1, 2, 0)\n",
        "            blur_np = blur.cpu().numpy().transpose(1, 2, 0)\n",
        "\n",
        "    # Calculate PSNR and SSIM\n",
        "            psnr_sharp = compare_psnr(denoised_np, sharp_np)\n",
        "            ssim_sharp = compare_ssim(denoised_np, sharp_np, multichannel=True)\n",
        "            psnr_blur = compare_psnr(denoised_np, blur_np)\n",
        "            ssim_blur = compare_ssim(denoised_np, blur_np, multichannel=True)\n",
        "\n",
        "            total_psnr_sharp += psnr_sharp\n",
        "            total_ssim_sharp += ssim_sharp\n",
        "            total_psnr_blur += psnr_blur\n",
        "            total_ssim_blur += ssim_blur\n",
        "            num_images += 1\n",
        "\n",
        "# Calculate average PSNR and SSIM\n",
        "avg_psnr_sharp = total_psnr_sharp / num_images\n",
        "avg_ssim_sharp = total_ssim_sharp / num_images\n",
        "avg_psnr_blur = total_psnr_blur / num_images\n",
        "avg_ssim_blur = total_ssim_blur / num_images\n",
        "\n",
        "print(f'Average PSNR (Denoised vs Sharp): {avg_psnr_sharp}')\n",
        "print(f'Average SSIM (Denoised vs Sharp): {avg_ssim_sharp}')\n",
        "print(f'Average PSNR (Denoised vs Blur): {avg_psnr_blur}')\n",
        "print(f'Average SSIM (Denoised vs Blur): {avg_ssim_blur}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvOThBoaooYn",
        "outputId": "dc9949a3-26b7-4e5c-eab5-6c21f9a4773f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-c2c78bbff395>:39: FutureWarning: `multichannel` is a deprecated argument name for `structural_similarity`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
            "  ssim_sharp = compare_ssim(denoised_np, sharp_np, multichannel=True)\n",
            "<ipython-input-8-c2c78bbff395>:41: FutureWarning: `multichannel` is a deprecated argument name for `structural_similarity`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
            "  ssim_blur = compare_ssim(denoised_np, blur_np, multichannel=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average PSNR (Denoised vs Sharp): 27.399835853064033\n",
            "Average SSIM (Denoised vs Sharp): 0.8841692714679836\n",
            "Average PSNR (Denoised vs Blur): 33.32209119904878\n",
            "Average SSIM (Denoised vs Blur): 0.97437595981317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# This part uses CPU calculations to save resources.\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_path = '/content/drive/MyDrive/model/DnCNN/denoising_DnCNN3.pth'\n",
        "\n",
        "model = DnCNN(channels=3)\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.eval()\n",
        "model = model.to(device)\n",
        "\n",
        "# Def dataload\n",
        "full_dataset = CustomDataset(root_dir='/content/datasets/train', transform=transform)\n",
        "full_loader = DataLoader(full_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Create picture directory\n",
        "output_dir_base = '/content/drive/MyDrive/image_output'\n",
        "denoised_dir = os.path.join(output_dir_base, 'denoised')\n",
        "sharp_resized_dir = os.path.join(output_dir_base, 'sharp_resized')\n",
        "sharp_original_dir = os.path.join(output_dir_base, 'sharp_original')\n",
        "blur_resized_dir = os.path.join(output_dir_base, 'blur_resized')\n",
        "\n",
        "os.makedirs(denoised_dir, exist_ok=True)\n",
        "os.makedirs(sharp_resized_dir, exist_ok=True)\n",
        "os.makedirs(sharp_original_dir, exist_ok=True)\n",
        "os.makedirs(blur_resized_dir, exist_ok=True)\n",
        "\n",
        "# Process and save images\n",
        "for i, (blur_img, sharp_img, sharp_img_original) in enumerate(full_loader):\n",
        "    blur_img = blur_img.to(device)\n",
        "\n",
        "    # Model prediction and inverse normalization\n",
        "    with torch.no_grad():\n",
        "        denoised_img = model(blur_img)\n",
        "    denoised_img = inv_normalize(denoised_img[0]).clamp(0, 1)\n",
        "\n",
        "    # Save denoised image\n",
        "    denoised_img_pil = transforms.ToPILImage()(denoised_img.cpu())\n",
        "    denoised_img_pil.save(os.path.join(denoised_dir, f'image_{i:04d}.png'))\n",
        "\n",
        "    # save resized sharp image\n",
        "    sharp_img = inv_normalize(sharp_img[0]).clamp(0, 1)\n",
        "    sharp_img_pil = transforms.ToPILImage()(sharp_img.cpu())\n",
        "    sharp_img_pil.save(os.path.join(sharp_resized_dir, f'image_{i:04d}.png'))\n",
        "\n",
        "    # save original image\n",
        "    sharp_img_original_pil = transforms.ToPILImage()(sharp_img_original.cpu().squeeze(0))\n",
        "    sharp_img_original_pil.save(os.path.join(sharp_original_dir, f'image_{i:04d}.png'))\n",
        "\n",
        "    # Save resized blur image\n",
        "    blur_img_resized = inv_normalize(blur_img.squeeze(0)).clamp(0, 1)\n",
        "    blur_img_resized_pil = transforms.ToPILImage()(blur_img_resized.cpu())\n",
        "    blur_img_resized_pil.save(os.path.join(blur_resized_dir, f'image_{i:04d}.png'))"
      ],
      "metadata": {
        "id": "ZHQlqUXlSQJ0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}