{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyVn9wlQz-xy",
        "outputId": "79a1dc08-87bf-4cbf-ebb2-1c556dd558d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjrKJmLf0DL_"
      },
      "outputs": [],
      "source": [
        "!unzip \"/content/drive/MyDrive/Colab Notebooks/GOPRO_Large.zip\" -d \"/content/datasets\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cT_Ps2zq0D3E"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from PIL import Image\n",
        "\n",
        "def transform_sharp(image):\n",
        "    # Provides a 720x1280 image for sharp_img\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((720, 1280)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])(image)\n",
        "\n",
        "def transform_blur(image):\n",
        "    # Provide a 360x640 image to blur_img\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((360, 640)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])(image)\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, mix_ratio=0.5):\n",
        "        self.root_dir = os.path.expanduser(root_dir)\n",
        "        self.mix_ratio = mix_ratio\n",
        "        self.samples = self._load_samples()\n",
        "\n",
        "    def _load_samples(self):\n",
        "        samples = []\n",
        "        for subdir in sorted(os.listdir(self.root_dir)):\n",
        "            subdir_path = os.path.join(self.root_dir, subdir)\n",
        "            if os.path.isdir(subdir_path):\n",
        "                sharp_imgs = sorted(os.listdir(os.path.join(subdir_path, 'sharp')))\n",
        "                blur_imgs = sorted(os.listdir(os.path.join(subdir_path, 'blur')))\n",
        "                blur_gamma_imgs = sorted(os.listdir(os.path.join(subdir_path, 'blur_gamma')))\n",
        "\n",
        "                for sharp_img, blur_img, blur_gamma_img in zip(sharp_imgs, blur_imgs, blur_gamma_imgs):\n",
        "                    sharp_path = os.path.join(subdir_path, 'sharp', sharp_img)\n",
        "                    blur_path = os.path.join(subdir_path, 'blur', blur_img)\n",
        "                    blur_gamma_path = os.path.join(subdir_path, 'blur_gamma', blur_gamma_img)\n",
        "                    samples.append((sharp_path, blur_path, blur_gamma_path))\n",
        "        return samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sharp_path, blur_path, blur_gamma_path = self.samples[idx]\n",
        "        sharp_img = Image.open(sharp_path).convert('RGB')\n",
        "        blur_img = Image.open(blur_path).convert('RGB')\n",
        "        blur_gamma_img = Image.open(blur_gamma_path).convert('RGB')\n",
        "\n",
        "        # Mix blur and blur_gamma images\n",
        "        blur_img = Image.blend(blur_img, blur_gamma_img, self.mix_ratio)\n",
        "        sharp_img = transform_sharp(sharp_img)\n",
        "        blur_img = transform_blur(blur_img)\n",
        "\n",
        "        return blur_img, sharp_img\n",
        "\n",
        "# Function used to save model output\n",
        "def save_model_output(output, filename):\n",
        "    output = output.cpu().detach()\n",
        "    output_img = transforms.ToPILImage()(output).convert('RGB')\n",
        "    output_img.save(filename)\n",
        "\n",
        "train_dataset = CustomDataset(root_dir='/content/datasets/train')\n",
        "# Split the data set into training set and validation set\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_subset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    num_workers=8\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_subset,\n",
        "    batch_size=2,\n",
        "    shuffle=False,\n",
        "    num_workers=8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LemiAQF1_PS"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n",
        "import torch.distributed as distance\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#def Perceptual Loss by MobileNet\n",
        "class PerceptualLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PerceptualLoss, self).__init__()\n",
        "        self.mobilenet = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.DEFAULT).features\n",
        "        self.mobilenet.eval()\n",
        "        for param in self.mobilenet.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        input_features = self.mobilenet(input)\n",
        "        target_features = self.mobilenet(target)\n",
        "        # Resize if necessary\n",
        "        if input_features.shape[2:] != target_features.shape[2:]:\n",
        "            input_features = F.interpolate(input_features, size=target_features.shape[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "        loss = nn.functional.mse_loss(input_features, target_features)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPDtwTGW1_4u"
      },
      "outputs": [],
      "source": [
        "#Define the DnCNN Model\n",
        "class DnCNN(nn.Module):\n",
        "    def __init__(self, channels, num_of_layers=17):\n",
        "        super(DnCNN, self).__init__()\n",
        "        kernel_size = 3\n",
        "        padding = 1\n",
        "        features = 64\n",
        "        layers = []\n",
        "\n",
        "        # Initial convolution layer\n",
        "        layers.append(nn.Conv2d(in_channels=channels, out_channels=features, kernel_size=kernel_size, padding=padding, bias=False))\n",
        "        layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        # Middle layers\n",
        "        for _ in range(num_of_layers - 2):\n",
        "            layers.append(nn.Conv2d(in_channels=features, out_channels=features, kernel_size=kernel_size, padding=padding, bias=False))\n",
        "            layers.append(nn.BatchNorm2d(features))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        # Final convolution layer\n",
        "        layers.append(nn.Conv2d(in_channels=features, out_channels=channels, kernel_size=kernel_size, padding=padding, bias=False))\n",
        "\n",
        "        self.dncnn = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.dncnn(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVPS84dN2E2A"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
        "\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysltBxCV3sF7"
      },
      "outputs": [],
      "source": [
        "# Definite U-Net\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        self.down4 = Down(512, 512)\n",
        "        self.up1 = Up(1024, 256)\n",
        "        self.up2 = Up(512, 128)\n",
        "        self.up3 = Up(256, 64)\n",
        "        self.up4 = Up(128, 64)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3hdVkFubWk9"
      },
      "outputs": [],
      "source": [
        "# Residual Block\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.conv1(x)\n",
        "        residual = self.relu(residual)\n",
        "        residual = self.conv2(residual)\n",
        "        return x + residual\n",
        "\n",
        "# EDSR Model, use scale_factor to choose scale\n",
        "class EDSR(nn.Module):\n",
        "    def __init__(self, scale_factor=2, num_channels=3, num_residual_blocks=16):\n",
        "        super(EDSR, self).__init__()\n",
        "        self.num_channels = num_channels\n",
        "\n",
        "        # First layer\n",
        "        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=9, padding=4)\n",
        "\n",
        "        # Residual blocks\n",
        "        self.residual_blocks = nn.Sequential(*[ResidualBlock(64) for _ in range(num_residual_blocks)])\n",
        "\n",
        "        # Second conv layer post residual blocks\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "\n",
        "        # Upsampling layers\n",
        "        self.upsampling = nn.Sequential(\n",
        "            nn.Conv2d(64, 256, kernel_size=3, padding=1),\n",
        "            nn.PixelShuffle(upscale_factor=scale_factor),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Output layer\n",
        "        self.conv3 = nn.Conv2d(64, num_channels, kernel_size=9, padding=4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        residual = out\n",
        "        out = self.residual_blocks(out)\n",
        "        out = self.conv2(out)\n",
        "        out = out + residual  # Element-wise sum\n",
        "        out = self.upsampling(out)\n",
        "        out = self.conv3(out)\n",
        "        return out\n",
        "\n",
        "#Create EDSR model instance\n",
        "model = EDSR(scale_factor=2, num_channels=3, num_residual_blocks=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1FEEJCX6BHU"
      },
      "outputs": [],
      "source": [
        "# Add SobelEdgeLoss\n",
        "class SobelEdgeLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SobelEdgeLoss, self).__init__()\n",
        "        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).view(1, 1, 3, 3)\n",
        "        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).view(1, 1, 3, 3)\n",
        "        self.sobel_x = sobel_x.repeat(3, 1, 1, 1).to(device)\n",
        "        self.sobel_y = sobel_y.repeat(3, 1, 1, 1).to(device)\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        input_x = F.conv2d(input, self.sobel_x, padding=1, groups=3)\n",
        "        input_y = F.conv2d(input, self.sobel_y, padding=1, groups=3)\n",
        "        target_x = F.conv2d(target, self.sobel_x, padding=1, groups=3)\n",
        "        target_y = F.conv2d(target, self.sobel_y, padding=1, groups=3)\n",
        "\n",
        "        input_edge = torch.sqrt(input_x ** 2 + input_y ** 2)\n",
        "        target_edge = torch.sqrt(target_x ** 2 + target_y ** 2)\n",
        "\n",
        "        loss = F.mse_loss(input_edge, target_edge)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkOJL_-73yJz"
      },
      "outputs": [],
      "source": [
        "dncnn = DnCNN(channels=3).to(device)\n",
        "unet = UNet(n_channels=3, n_classes=3).to(device)\n",
        "edsr = EDSR(scale_factor=2, num_channels=3, num_residual_blocks=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSuMUbVa38eV"
      },
      "outputs": [],
      "source": [
        " dncnn.load_state_dict(torch.load('/content/drive/MyDrive/model/DnCNN/denoising_DnCNN3.pth'))\n",
        " unet.load_state_dict(torch.load('/content/drive/MyDrive/model/UNet/deblured_UNet4.pth'))\n",
        " edsr.load_state_dict(torch.load('/content/drive/MyDrive/model/EDSR/SuperResolution_EDSR.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN28tftv3_sL"
      },
      "outputs": [],
      "source": [
        "class JointModel(nn.Module):\n",
        "    def __init__(self, dncnn, unet, edsr):\n",
        "        super(JointModel, self).__init__()\n",
        "        self.dncnn = dncnn\n",
        "        self.unet = unet\n",
        "        self.edsr = edsr\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dncnn(x)  # denoised\n",
        "        x = self.unet(x)  # deblured\n",
        "        x = self.edsr(x)  # super resolution\n",
        "        return x\n",
        "\n",
        "joint_model = JointModel(dncnn, unet, edsr).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBSfQVH85osd"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "# Creat model and Adam optimizer\n",
        "mse_criterion = nn.MSELoss()\n",
        "perceptual_criterion = PerceptualLoss().to(device)\n",
        "optimizer = torch.optim.Adam(joint_model.parameters(), lr=0.00001)\n",
        "\n",
        "# Initialize the ReduceLROnPlateau scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=5, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1g9mpq6AcZt"
      },
      "outputs": [],
      "source": [
        "#Defining Loss Function Weights\n",
        "mse_weight = 1.0\n",
        "perceptual_weight = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBym4ur16LNJ"
      },
      "outputs": [],
      "source": [
        "# Train model\n",
        "def train_model(joint_model, train_loader, val_loader, mse_criterion, perceptual_criterion, optimizer, num_epochs=300, early_stopping_tolerance=15):\n",
        "    best_val_loss = float('inf')\n",
        "    no_improvement_count = 0  # Early stopping counter\n",
        "\n",
        "    sobel_criterion = SobelEdgeLoss().to(device)\n",
        "    sobel_weight = 0.05\n",
        "\n",
        "    scaler = GradScaler()\n",
        "    for epoch in range(num_epochs):\n",
        "        joint_model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for blur_img, transformed_sharp_img in train_loader:\n",
        "            blur_img = blur_img.to(device)\n",
        "            transformed_sharp_img = transformed_sharp_img.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "           # Performing forward propagation using the autocast context\n",
        "            with autocast():\n",
        "                outputs = joint_model(blur_img)\n",
        "                mse_loss = mse_criterion(outputs, transformed_sharp_img)\n",
        "                perceptual_loss = perceptual_criterion(outputs, transformed_sharp_img)\n",
        "                sobel_loss = sobel_criterion(outputs, transformed_sharp_img)\n",
        "                total_loss = mse_weight * mse_loss + perceptual_weight * perceptual_loss + sobel_weight * sobel_loss\n",
        "\n",
        "            # Performing backward propagation and optimization using GradScaler\n",
        "            scaler.scale(total_loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += total_loss.item() * blur_img.size(0)\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "        # Validation test\n",
        "        joint_model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for blur_img, transformed_sharp_img in val_loader:\n",
        "                blur_img = blur_img.to(device)\n",
        "                transformed_sharp_img = transformed_sharp_img.to(device)\n",
        "                outputs = joint_model(blur_img)\n",
        "                mse_loss = mse_criterion(outputs, transformed_sharp_img)\n",
        "                perceptual_loss = perceptual_criterion(outputs, transformed_sharp_img)\n",
        "                sobel_loss = sobel_criterion(outputs, transformed_sharp_img)\n",
        "                total_loss = mse_weight * mse_loss + perceptual_weight * perceptual_loss + sobel_weight * sobel_loss\n",
        "                val_loss += total_loss.item() * blur_img.size(0)\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        # Early stopping check\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            no_improvement_count = 0\n",
        "        else:\n",
        "            no_improvement_count += 1\n",
        "            if no_improvement_count >= early_stopping_tolerance:\n",
        "                print(\"Stopping early due to no improvement in validation loss\")\n",
        "                break\n",
        "\n",
        "    return joint_model\n",
        "\n",
        "train_model(joint_model, train_loader, val_loader, mse_criterion, perceptual_criterion, optimizer, num_epochs=300, early_stopping_tolerance=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvXKjWswHLwB",
        "outputId": "3fdffc4b-3c3f-4b9d-a2db-29a174214ec9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to /content/drive/MyDrive/model/joint_model.pth.\n"
          ]
        }
      ],
      "source": [
        "# Save model\n",
        "if torch.cuda.is_available() and torch.cuda.current_device() == 0:\n",
        "    model_path = '/content/drive/MyDrive/model/joint_model.pth'\n",
        "    model_dir = os.path.expanduser(os.path.dirname(model_path))\n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "\n",
        "    torch.save(joint_model.state_dict(), os.path.expanduser(model_path))\n",
        "    print(f\"Model saved to {model_path}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x__hIGQ_SIaD"
      },
      "outputs": [],
      "source": [
        "# Inverse normalization transformation\n",
        "inv_normalize = transforms.Normalize(\n",
        "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
        "    std=[1/0.229, 1/0.224, 1/0.225]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuk19TV6SJgS"
      },
      "outputs": [],
      "source": [
        "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
        "from skimage.metrics import structural_similarity as compare_ssim\n",
        "from skimage import img_as_float\n",
        "import torch\n",
        "\n",
        "# Initialize the sum of PSNR and SSIM\n",
        "total_psnr_sharp = 0\n",
        "total_ssim_sharp = 0\n",
        "total_psnr_blur = 0\n",
        "total_ssim_blur = 0\n",
        "num_images = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for blur_img, sharp_img in val_loader:\n",
        "        blur_img = blur_img.to(device)\n",
        "        sharp_img = sharp_img.to(device)\n",
        "\n",
        "        deblured_img = joint_model(blur_img)\n",
        "\n",
        "# Traverse the image to calculate PSNR and SSIM\n",
        "        for i in range(blur_img.size(0)):\n",
        "            deblured = inv_normalize(deblured_img[i]).clamp(0, 1)\n",
        "            sharp = inv_normalize(sharp_img[i]).clamp(0, 1)\n",
        "            blur = inv_normalize(blur_img[i]).clamp(0, 1)\n",
        "\n",
        "            deblured_np = deblured.cpu().numpy().transpose(1, 2, 0)\n",
        "            sharp_np = sharp.cpu().numpy().transpose(1, 2, 0)\n",
        "            blur_np = blur.cpu().numpy().transpose(1, 2, 0)\n",
        "\n",
        "    # Calculate PSNR and SSIM\n",
        "            psnr_sharp = compare_psnr(deblured_np, sharp_np)\n",
        "            ssim_sharp = compare_ssim(deblured_np, sharp_np, multichannel=True)\n",
        "\n",
        "            total_psnr_sharp += psnr_sharp\n",
        "            total_ssim_sharp += ssim_sharp\n",
        "            num_images += 1\n",
        "\n",
        "# Calculate average PSNR and SSIM\n",
        "avg_psnr_sharp = total_psnr_sharp / num_images\n",
        "avg_ssim_sharp = total_ssim_sharp / num_images\n",
        "\n",
        "\n",
        "print(f'Average PSNR (Deblured vs Sharp): {avg_psnr_sharp}')\n",
        "print(f'Average SSIM (Deblured vs Sharp): {avg_ssim_sharp}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vD1b2RyNPhl-"
      },
      "outputs": [],
      "source": [
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import os\n",
        "import torch\n",
        "from torchvision import transforms, utils\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "\n",
        "def process_blur_images(joint_model_path, dataset_root, output_folder):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    joint_model.load_state_dict(torch.load(joint_model_path, map_location=device))\n",
        "    joint_model.eval()\n",
        "    joint_model.to(device)\n",
        "\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    img_counter = 1\n",
        "    for subdir in sorted(os.listdir(dataset_root)):\n",
        "        blur_folder = os.path.join(dataset_root, subdir, 'blur')\n",
        "        if os.path.isdir(blur_folder):\n",
        "            for blur_img_name in sorted(os.listdir(blur_folder)):\n",
        "                blur_img_path = os.path.join(blur_folder, blur_img_name)\n",
        "                blur_img = Image.open(blur_img_path).convert('RGB')\n",
        "                blur_img_tensor = transform_blur(blur_img).unsqueeze(0).to(device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    output = joint_model(blur_img_tensor)\n",
        "\n",
        "                output = inv_normalize(output.squeeze(0)).cpu()\n",
        "\n",
        "                output_filename = f'img_{img_counter:04d}.png'\n",
        "                output_image_path = os.path.join(output_folder, output_filename)\n",
        "                utils.save_image(output, output_image_path)\n",
        "\n",
        "                img_counter += 1\n",
        "\n",
        "process_blur_images('/content/drive/MyDrive/model/joint_model.pth', '/content/datasets/train', '/content/image/processed_images')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHJKG51XC3EM"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageFilter\n",
        "import os\n",
        "\n",
        "def apply_unsharp_mask_and_save(input_folder, output_folder, radius=2, percent=150, threshold=3):\n",
        "    # Create the output folder if it does not exist\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Process and save each image\n",
        "    for img_name in os.listdir(input_folder):\n",
        "        img_path = os.path.join(input_folder, img_name)\n",
        "        with Image.open(img_path) as img:\n",
        "            # Apply Unsharp Mask\n",
        "            sharpened_img = img.filter(ImageFilter.UnsharpMask(radius=radius, percent=percent, threshold=threshold))\n",
        "\n",
        "            # Save the sharpened image\n",
        "            output_img_path = os.path.join(output_folder, img_name)\n",
        "            sharpened_img.save(output_img_path)\n",
        "\n",
        "# Use the function\n",
        "apply_unsharp_mask_and_save('/content/image/processed_images', '/content/image/sharpened_images')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHpPKv2bHj4k"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "source_folder = '/content/image'\n",
        "zip_file_path = '/content/drive/MyDrive/image.zip'\n",
        "shutil.make_archive(zip_file_path[:-4], 'zip', source_folder)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNhiw5ghMSXB8ZFFRn6mO/+"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}