{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyPJJFYD2/kkzHkIw6zAu2l3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ldjjlws0h4FA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/image_output.zip\" -d \"/content/datasets\""
      ],
      "metadata": {
        "id": "4NVEyvAkob2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from PIL import Image\n",
        "\n",
        "class SuperResolutionDataset(Dataset):\n",
        "    def __init__(self, hr_dir, lr_dir, hr_transform=None, lr_transform=None):\n",
        "        self.hr_dir = hr_dir\n",
        "        self.lr_dir = lr_dir\n",
        "        self.hr_transform = hr_transform\n",
        "        self.lr_transform = lr_transform\n",
        "        self.filenames = [f for f in os.listdir(lr_dir) if os.path.isfile(os.path.join(hr_dir, f))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        hr_path = os.path.join(self.hr_dir, self.filenames[idx])\n",
        "        lr_path = os.path.join(self.lr_dir, self.filenames[idx])\n",
        "\n",
        "        hr_image = Image.open(hr_path).convert('RGB')\n",
        "        lr_image = Image.open(lr_path).convert('RGB')\n",
        "\n",
        "        if self.hr_transform:\n",
        "            hr_image = self.hr_transform(hr_image)\n",
        "        if self.lr_transform:\n",
        "            lr_image = self.lr_transform(lr_image)\n",
        "\n",
        "        return lr_image, hr_image\n",
        "\n",
        "# Conversion of high-resolution and low-resolution images\n",
        "hr_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "lr_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Creat Datasets\n",
        "dataset = SuperResolutionDataset(\n",
        "    hr_dir='/content/datasets/sharp_original',\n",
        "    lr_dir='/content/datasets/deblured',\n",
        "    hr_transform=hr_transform,\n",
        "    lr_transform=lr_transform\n",
        ")\n",
        "\n",
        "# Split datasets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Definite dataloader\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=8)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=8)"
      ],
      "metadata": {
        "id": "PXEni3g6ocpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Residual Block\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.conv1(x)\n",
        "        residual = self.relu(residual)\n",
        "        residual = self.conv2(residual)\n",
        "        return x + residual\n",
        "\n",
        "# EDSR Model, use scale_factor to choose scale\n",
        "class EDSR(nn.Module):\n",
        "    def __init__(self, scale_factor=2, num_channels=3, num_residual_blocks=16):\n",
        "        super(EDSR, self).__init__()\n",
        "        self.num_channels = num_channels\n",
        "\n",
        "        # First layer\n",
        "        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=9, padding=4)\n",
        "\n",
        "        # Residual blocks\n",
        "        self.residual_blocks = nn.Sequential(*[ResidualBlock(64) for _ in range(num_residual_blocks)])\n",
        "\n",
        "        # Second conv layer post residual blocks\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "\n",
        "        # Upsampling layers\n",
        "        self.upsampling = nn.Sequential(\n",
        "            nn.Conv2d(64, 256, kernel_size=3, padding=1),\n",
        "            nn.PixelShuffle(upscale_factor=scale_factor),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Output layer\n",
        "        self.conv3 = nn.Conv2d(64, num_channels, kernel_size=9, padding=4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        residual = out\n",
        "        out = self.residual_blocks(out)\n",
        "        out = self.conv2(out)\n",
        "        out = out + residual  # Element-wise sum\n",
        "        out = self.upsampling(out)\n",
        "        out = self.conv3(out)\n",
        "        return out\n",
        "\n",
        "#Create EDSR model instance\n",
        "model = EDSR(scale_factor=2, num_channels=3, num_residual_blocks=16)"
      ],
      "metadata": {
        "id": "1xZZb0W9o-l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n",
        "import torch.distributed as distance\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#def Perceptual Loss by MobileNet\n",
        "class PerceptualLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PerceptualLoss, self).__init__()\n",
        "        self.mobilenet = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.DEFAULT).features\n",
        "        self.mobilenet.eval()\n",
        "        for param in self.mobilenet.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        input_features = self.mobilenet(input)\n",
        "        target_features = self.mobilenet(target)\n",
        "        # Resize if necessary\n",
        "        if input_features.shape[2:] != target_features.shape[2:]:\n",
        "            input_features = F.interpolate(input_features, size=target_features.shape[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "        loss = nn.functional.mse_loss(input_features, target_features)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "5LB76dx1qUd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "# Creat model and Adam optimizer\n",
        "SuperResolution_model = EDSR(scale_factor=2, num_channels=3, num_residual_blocks=16).to(device)\n",
        "mse_criterion = nn.MSELoss()\n",
        "perceptual_criterion = PerceptualLoss().to(device)\n",
        "optimizer = torch.optim.Adam(SuperResolution_model.parameters(), lr=0.0001)\n",
        "\n",
        "# Initialize the ReduceLROnPlateau scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=3, verbose=True)"
      ],
      "metadata": {
        "id": "tonGtO01qXRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining Loss Function Weights\n",
        "mse_weight = 1.0\n",
        "perceptual_weight = 0.1"
      ],
      "metadata": {
        "id": "yCQocVIgqnja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "def train_model(model, train_loader, val_loader, mse_criterion, perceptual_criterion, optimizer, num_epochs=100, early_stopping_tolerance=8):\n",
        "    best_val_loss = float('inf')\n",
        "    no_improvement_count = 0  # Early stopping counter\n",
        "\n",
        "    scaler = GradScaler()\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for lr_image, hr_image in train_loader:\n",
        "            lr_image = lr_image.to(device)\n",
        "            hr_image = hr_image.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "           # Performing forward propagation using the autocast context\n",
        "            with autocast():\n",
        "                outputs = model(lr_image)\n",
        "                mse_loss = mse_criterion(outputs, hr_image)\n",
        "                perceptual_loss = perceptual_criterion(outputs, hr_image)\n",
        "                total_loss = mse_weight * mse_loss + perceptual_weight * perceptual_loss\n",
        "\n",
        "            # Performing backward propagation and optimization using GradScaler\n",
        "            scaler.scale(total_loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += total_loss.item() * lr_image.size(0)\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "        # Validation test\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for lr_image, hr_image in val_loader:\n",
        "                lr_image = lr_image.to(device)\n",
        "                hr_image = hr_image.to(device)\n",
        "                outputs = model(lr_image)\n",
        "                mse_loss = mse_criterion(outputs, hr_image)\n",
        "                perceptual_loss = perceptual_criterion(outputs, hr_image)\n",
        "                total_loss = mse_weight * mse_loss + perceptual_weight * perceptual_loss\n",
        "                val_loss += total_loss.item() * lr_image.size(0)\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        # Early stopping check\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            no_improvement_count = 0\n",
        "        else:\n",
        "            no_improvement_count += 1\n",
        "            if no_improvement_count >= early_stopping_tolerance:\n",
        "                print(\"Stopping early due to no improvement in validation loss\")\n",
        "                break\n",
        "\n",
        "    return model\n",
        "\n",
        "train_model(SuperResolution_model, train_loader, val_loader, mse_criterion, perceptual_criterion, optimizer, num_epochs=100, early_stopping_tolerance=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icff-pJ_q6fg",
        "outputId": "9afa7624-aa86-4f39-c049-11d7bcaa0001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Training Loss: 0.2146, Validation Loss: 0.1678\n",
            "Epoch 2/100, Training Loss: 0.1578, Validation Loss: 0.1567\n",
            "Epoch 3/100, Training Loss: 0.1524, Validation Loss: 0.1532\n",
            "Epoch 4/100, Training Loss: 0.1494, Validation Loss: 0.1516\n",
            "Epoch 5/100, Training Loss: 0.1479, Validation Loss: 0.1523\n",
            "Epoch 6/100, Training Loss: 0.1465, Validation Loss: 0.1489\n",
            "Epoch 7/100, Training Loss: 0.1455, Validation Loss: 0.1466\n",
            "Epoch 8/100, Training Loss: 0.1446, Validation Loss: 0.1470\n",
            "Epoch 9/100, Training Loss: 0.1441, Validation Loss: 0.1459\n",
            "Epoch 10/100, Training Loss: 0.1433, Validation Loss: 0.1458\n",
            "Epoch 11/100, Training Loss: 0.1427, Validation Loss: 0.1448\n",
            "Epoch 12/100, Training Loss: 0.1420, Validation Loss: 0.1444\n",
            "Epoch 13/100, Training Loss: 0.1415, Validation Loss: 0.1439\n",
            "Epoch 14/100, Training Loss: 0.1411, Validation Loss: 0.1435\n",
            "Epoch 15/100, Training Loss: 0.1404, Validation Loss: 0.1435\n",
            "Epoch 16/100, Training Loss: 0.1401, Validation Loss: 0.1427\n",
            "Epoch 17/100, Training Loss: 0.1396, Validation Loss: 0.1436\n",
            "Epoch 18/100, Training Loss: 0.1393, Validation Loss: 0.1419\n",
            "Epoch 19/100, Training Loss: 0.1387, Validation Loss: 0.1418\n",
            "Epoch 20/100, Training Loss: 0.1384, Validation Loss: 0.1410\n",
            "Epoch 21/100, Training Loss: 0.1380, Validation Loss: 0.1404\n",
            "Epoch 22/100, Training Loss: 0.1374, Validation Loss: 0.1410\n",
            "Epoch 23/100, Training Loss: 0.1372, Validation Loss: 0.1401\n",
            "Epoch 24/100, Training Loss: 0.1367, Validation Loss: 0.1399\n",
            "Epoch 25/100, Training Loss: 0.1365, Validation Loss: 0.1394\n",
            "Epoch 26/100, Training Loss: 0.1360, Validation Loss: 0.1397\n",
            "Epoch 27/100, Training Loss: 0.1357, Validation Loss: 0.1397\n",
            "Epoch 28/100, Training Loss: 0.1353, Validation Loss: 0.1385\n",
            "Epoch 29/100, Training Loss: 0.1348, Validation Loss: 0.1391\n",
            "Epoch 30/100, Training Loss: 0.1345, Validation Loss: 0.1386\n",
            "Epoch 31/100, Training Loss: 0.1343, Validation Loss: 0.1375\n",
            "Epoch 32/100, Training Loss: 0.1337, Validation Loss: 0.1382\n",
            "Epoch 33/100, Training Loss: 0.1336, Validation Loss: 0.1385\n",
            "Epoch 34/100, Training Loss: 0.1332, Validation Loss: 0.1375\n",
            "Epoch 35/100, Training Loss: 0.1331, Validation Loss: 0.1372\n",
            "Epoch 36/100, Training Loss: 0.1327, Validation Loss: 0.1371\n",
            "Epoch 37/100, Training Loss: 0.1323, Validation Loss: 0.1369\n",
            "Epoch 38/100, Training Loss: 0.1321, Validation Loss: 0.1369\n",
            "Epoch 39/100, Training Loss: 0.1317, Validation Loss: 0.1375\n",
            "Epoch 40/100, Training Loss: 0.1315, Validation Loss: 0.1361\n",
            "Epoch 41/100, Training Loss: 0.1313, Validation Loss: 0.1363\n",
            "Epoch 42/100, Training Loss: 0.1310, Validation Loss: 0.1377\n",
            "Epoch 43/100, Training Loss: 0.1307, Validation Loss: 0.1359\n",
            "Epoch 44/100, Training Loss: 0.1304, Validation Loss: 0.1354\n",
            "Epoch 45/100, Training Loss: 0.1300, Validation Loss: 0.1353\n",
            "Epoch 46/100, Training Loss: 0.1301, Validation Loss: 0.1364\n",
            "Epoch 47/100, Training Loss: 0.1299, Validation Loss: 0.1352\n",
            "Epoch 48/100, Training Loss: 0.1295, Validation Loss: 0.1345\n",
            "Epoch 49/100, Training Loss: 0.1292, Validation Loss: 0.1360\n",
            "Epoch 50/100, Training Loss: 0.1290, Validation Loss: 0.1352\n",
            "Epoch 51/100, Training Loss: 0.1285, Validation Loss: 0.1347\n",
            "Epoch 52/100, Training Loss: 0.1283, Validation Loss: 0.1348\n",
            "Epoch 00052: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch 53/100, Training Loss: 0.1261, Validation Loss: 0.1327\n",
            "Epoch 54/100, Training Loss: 0.1257, Validation Loss: 0.1326\n",
            "Epoch 55/100, Training Loss: 0.1256, Validation Loss: 0.1327\n",
            "Epoch 56/100, Training Loss: 0.1256, Validation Loss: 0.1326\n",
            "Epoch 57/100, Training Loss: 0.1255, Validation Loss: 0.1326\n",
            "Epoch 58/100, Training Loss: 0.1255, Validation Loss: 0.1325\n",
            "Epoch 59/100, Training Loss: 0.1254, Validation Loss: 0.1325\n",
            "Epoch 60/100, Training Loss: 0.1253, Validation Loss: 0.1325\n",
            "Epoch 61/100, Training Loss: 0.1253, Validation Loss: 0.1325\n",
            "Epoch 62/100, Training Loss: 0.1253, Validation Loss: 0.1325\n",
            "Epoch 63/100, Training Loss: 0.1252, Validation Loss: 0.1325\n",
            "Epoch 64/100, Training Loss: 0.1252, Validation Loss: 0.1324\n",
            "Epoch 65/100, Training Loss: 0.1251, Validation Loss: 0.1325\n",
            "Epoch 66/100, Training Loss: 0.1251, Validation Loss: 0.1325\n",
            "Epoch 67/100, Training Loss: 0.1250, Validation Loss: 0.1324\n",
            "Epoch 68/100, Training Loss: 0.1250, Validation Loss: 0.1324\n",
            "Epoch 69/100, Training Loss: 0.1249, Validation Loss: 0.1323\n",
            "Epoch 70/100, Training Loss: 0.1249, Validation Loss: 0.1324\n",
            "Epoch 71/100, Training Loss: 0.1248, Validation Loss: 0.1324\n",
            "Epoch 72/100, Training Loss: 0.1248, Validation Loss: 0.1323\n",
            "Epoch 73/100, Training Loss: 0.1248, Validation Loss: 0.1323\n",
            "Epoch 74/100, Training Loss: 0.1247, Validation Loss: 0.1323\n",
            "Epoch 75/100, Training Loss: 0.1247, Validation Loss: 0.1322\n",
            "Epoch 76/100, Training Loss: 0.1246, Validation Loss: 0.1321\n",
            "Epoch 77/100, Training Loss: 0.1246, Validation Loss: 0.1323\n",
            "Epoch 78/100, Training Loss: 0.1246, Validation Loss: 0.1322\n",
            "Epoch 79/100, Training Loss: 0.1245, Validation Loss: 0.1322\n",
            "Epoch 80/100, Training Loss: 0.1245, Validation Loss: 0.1322\n",
            "Epoch 00080: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Epoch 81/100, Training Loss: 0.1242, Validation Loss: 0.1320\n",
            "Epoch 82/100, Training Loss: 0.1241, Validation Loss: 0.1320\n",
            "Epoch 83/100, Training Loss: 0.1241, Validation Loss: 0.1320\n",
            "Epoch 84/100, Training Loss: 0.1241, Validation Loss: 0.1320\n",
            "Epoch 85/100, Training Loss: 0.1241, Validation Loss: 0.1320\n",
            "Epoch 00085: reducing learning rate of group 0 to 1.0000e-07.\n",
            "Epoch 86/100, Training Loss: 0.1241, Validation Loss: 0.1320\n",
            "Epoch 87/100, Training Loss: 0.1240, Validation Loss: 0.1320\n",
            "Epoch 88/100, Training Loss: 0.1240, Validation Loss: 0.1320\n",
            "Epoch 89/100, Training Loss: 0.1240, Validation Loss: 0.1320\n",
            "Epoch 00089: reducing learning rate of group 0 to 1.0000e-08.\n",
            "Epoch 90/100, Training Loss: 0.1240, Validation Loss: 0.1320\n",
            "Epoch 91/100, Training Loss: 0.1240, Validation Loss: 0.1320\n",
            "Epoch 92/100, Training Loss: 0.1240, Validation Loss: 0.1320\n",
            "Epoch 93/100, Training Loss: 0.1240, Validation Loss: 0.1320\n",
            "Epoch 94/100, Training Loss: 0.1240, Validation Loss: 0.1320\n",
            "Stopping early due to no improvement in validation loss\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EDSR(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
              "  (residual_blocks): Sequential(\n",
              "    (0): ResidualBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (1): ResidualBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (2): ResidualBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (3): ResidualBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (4): ResidualBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (5): ResidualBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (6): ResidualBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (7): ResidualBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (8): ResidualBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (9): ResidualBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (10): ResidualBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (11): ResidualBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (12): ResidualBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (13): ResidualBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (14): ResidualBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (15): ResidualBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (upsampling): Sequential(\n",
              "    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): PixelShuffle(upscale_factor=2)\n",
              "    (2): ReLU(inplace=True)\n",
              "  )\n",
              "  (conv3): Conv2d(64, 3, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "if torch.cuda.is_available() and torch.cuda.current_device() == 0:\n",
        "    model_path = '/content/drive/MyDrive/model/EDSR/SuperResolution_EDSR.pth'\n",
        "    model_dir = os.path.expanduser(os.path.dirname(model_path))\n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "\n",
        "    torch.save(SuperResolution_model.state_dict(), os.path.expanduser(model_path))\n",
        "    print(f\"Model saved to {model_path}.\")"
      ],
      "metadata": {
        "id": "JMRHIwEBsK_P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1a89dac-31e0-49a3-89ed-ada9d339276b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/drive/MyDrive/model/EDSR/SuperResolution_EDSR.pth.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inverse normalization transformation\n",
        "inv_normalize = transforms.Normalize(\n",
        "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
        "    std=[1/0.229, 1/0.224, 1/0.225]\n",
        ")"
      ],
      "metadata": {
        "id": "581QPmNeSfs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
        "from skimage.metrics import structural_similarity as compare_ssim\n",
        "from skimage import img_as_float\n",
        "import torch\n",
        "\n",
        "# Initialize the sum of PSNR and SSIM\n",
        "total_psnr = 0\n",
        "total_ssim = 0\n",
        "num_images = 0\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_path = '/content/drive/MyDrive/model/EDSR/SuperResolution_EDSR.pth'\n",
        "\n",
        "model = EDSR(scale_factor=2, num_channels=3, num_residual_blocks=16)\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for lr_image, hr_image in val_loader:\n",
        "        lr_image = lr_image.to(device)\n",
        "        hr_image = hr_image.to(device)\n",
        "\n",
        "        SuperResolution = model(lr_image)\n",
        "\n",
        "        # Traverse the image to calculate PSNR and SSIM\n",
        "        for i in range(lr_image.size(0)):\n",
        "            SuperResolution_img = inv_normalize(SuperResolution[i]).clamp(0, 1)\n",
        "            hr_img = inv_normalize(hr_image[i]).clamp(0, 1)\n",
        "\n",
        "            SuperResolution_np = SuperResolution_img.cpu().numpy().transpose(1, 2, 0)\n",
        "            hr_np = hr_img.cpu().numpy().transpose(1, 2, 0)\n",
        "\n",
        "            # Calculate PSNR and SSIM\n",
        "            psnr = compare_psnr(SuperResolution_np, hr_np)\n",
        "            ssim = compare_ssim(SuperResolution_np, hr_np, multichannel=True)\n",
        "\n",
        "            total_psnr += psnr\n",
        "            total_ssim += ssim\n",
        "            num_images += 1\n",
        "\n",
        "# Calculate average PSNR and SSIM\n",
        "avg_psnr = total_psnr / num_images\n",
        "avg_ssim = total_ssim / num_images\n",
        "\n",
        "print(f'Average PSNR: {avg_psnr}')\n",
        "print(f'Average SSIM: {avg_ssim}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeIyrn0KSjJi",
        "outputId": "9dac0a67-1fe1-413d-a7ca-4cee8287add0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-d6198716da77>:37: FutureWarning: `multichannel` is a deprecated argument name for `structural_similarity`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
            "  ssim = compare_ssim(SuperResolution_np, hr_np, multichannel=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average PSNR: 25.400591350396446\n",
            "Average SSIM: 0.8580290584552883\n"
          ]
        }
      ]
    }
  ]
}